{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqOQgnEwo8PWj66hMnnEop"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5월 19일"
      ],
      "metadata": {
        "id": "s2ZDPfBhdF8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코딩 테스트\n",
        "### 코딩테스트 연습하기"
      ],
      "metadata": {
        "id": "l3jj-EtZc9MP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2sLITQ9YiAB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "-------------------------------------------\n",
        "- \n",
        "- \n",
        "- \n",
        "- \n"
      ],
      "metadata": {
        "id": "-sFUG_8rdIm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터분석\n",
        "\n"
      ],
      "metadata": {
        "id": "De_ylXvDc_22"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CUgc_dZKdFK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 랜덤포레스트 복습\n",
        "\n",
        "#### 부스팅(boosting) 계열\n",
        "\n",
        "boosting : 약한 학습기를 강하게 만들어주는것\n",
        "\n",
        "- 틀린것에 가중치를 줌\n",
        "- 배깅 : 데이터 뻥튀기 parallel\n",
        "- 부스팅 : 순차적으로 학습하고 조합하여 강한 부스팅을 만들어감.\n",
        "- ada boosting\n",
        "- gradient boosting\n"
      ],
      "metadata": {
        "id": "5a-lnxltdKSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Algorithm(GBM)\n",
        "\n",
        "\n",
        "- 앙상블 방법론 중 부스팅 계열에 속하는 알고리즘\n",
        "\n",
        "- Tabular format 데이터에 대한 예측에서 엄청난 성능을 보여주고, 머신러닝 알고리즘 중에서 가장 예측 성능이 높다고 알려진 알고리즘\n",
        "\n",
        "- Kaggle에서의 다수의 우승모델은 XGBoost,LightGBM, CatBoost 같은 파이썬 패키지\n",
        "\n",
        "- GBM은 계산량이 상당히 많아 필요한 알고리즘이기 때문에, 이를 하드웨어를 효율적으로 구현하는 것이 필요. 위 패키지들은 모두 GBM을 효율적으로 구현하려고 한 패키지\n",
        "\n",
        "<figure>\n",
        "    <img src='https://t1.daumcdn.net/cfile/tistory/995D67335C46BA4114' width=700 height=300>\n",
        "</figure>\n",
        "\n",
        "출처 : https://3months.tistory.com/368#:~:text=Gradient%20Boosting%20Algorithm%20(GBM)%EC%9D%80,%EA%B3%84%EC%97%B4%EC%97%90%20%EC%86%8D%ED%95%98%EB%8A%94%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9E%85%EB%8B%88%EB%8B%A4.&text=%EA%B7%B8%EB%A0%87%EA%B8%B0%20%EB%95%8C%EB%AC%B8%EC%97%90%20Gradient%20Boosting%20Algorithm%EC%9D%84%20%EA%B5%AC%ED%98%84%ED%95%9C%20%ED%8C%A8%ED%82%A4%EC%A7%80%EB%93%A4%EC%9D%B4%20%EB%A7%8E%EC%8A%B5%EB%8B%88%EB%8B%A4.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Gradient를 이용하여 Boosting하는 알고리즘\n",
        "\n",
        "\n",
        "<figure>\n",
        "    <img src='https://t1.daumcdn.net/cfile/tistory/995D67335C46BA4114' width=700 height=300>\n",
        "</figure>\n",
        "\n",
        "\n",
        "## GBM의 직관적인 이해\n",
        "\n",
        " - 가장 쉬운 방법은 Residual Fitting으로 이해하는 것. 아주 간단한 모델 A를 통해 y를 예측하고 남은 잔차(Residual)을 다시 B라는 모델을 예측을 통해 A+B 모델을 통해 y를 예측한다면 A보다 나은 B 모델을 만들 수 있음.\n",
        "\n",
        " - 이러한 방법을 계속하면 잔차는 계속해서 줄어들게 되고, training set을 잘 설명하는 예측 모형을 만들 수 있게 됨.\n",
        "\n",
        " - 이러한 방식은 bias는 상당히 줄일 수 있어도, 과적합이 일어날 수 있다는 단점이 있음.\n",
        "\n",
        " - GBM을 사용할 때는 Sampling, penalizing 등의 regularizaion 테크닉을 이용하여 더 advanced 된 모델을 이용하는 것이 보편적\n",
        "\n",
        "\n",
        "<figure>\n",
        "    <img src='https://t1.daumcdn.net/cfile/tistory/99A9FC375C46C0201B' width=700 height=300>\n",
        "</figure>\n",
        "\n",
        "출처 :  https://3months.tistory.com/368?category=756964\n",
        "\n",
        "- 점점 잔차를 줄여나가는 것으로 학습.\n",
        "\n",
        "<figure>\n",
        "    <img src='https://miro.medium.com/max/2000/1*xQXPUxSnBd1lOxjLoePoVQ.png' width=700 height=300>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "Dl_Ta80SzQ5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실습 문제\n",
        "\n",
        " - Otto Group Product Classification Challenge (https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
        "\n",
        " - Cardiovascular Disease(https://www.kaggle.com/sulianova/cardiovascular-disease-dataset)"
      ],
      "metadata": {
        "id": "fvA72zxHzVi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼 파라미터\n",
        "\n",
        "- n_estimator: 결정 트리의 개수.  default는 10, 많을 수록 좋은 성능이 나올 수도 있지만, 무조건적인 것은 아님\n",
        "- max_features : 데이터의 feature를 참조할 비율, 개수를 뜻합니다. default는 auto입니다.\n",
        "\n",
        "- max_depth : 트리의 깊이를 뜻함.\n",
        "- min_samples_leaf : 리프노드가 되기 위한 최소한의 샘플 데이터 수\n",
        "- min_samples_leaf : 리프노드가 되기 위한 최소한의 샘플 데이터 수\n",
        "- min_samples_split : 리프노드가 되기 위한 최소한의 샘플 데이터 수\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## RandomForest에서의 하이퍼 파라미터와 같음.\n",
        "\n",
        "## Graident Boosting에서 사용하는 하이퍼 파라미터는 아래와 같음.\n",
        "\n",
        "- loss : gradient descent에서 사용할 비용 함수. 특별한 이유가 없으면 deafult로 감\n",
        "-  learning_rate : 값이 너무 적으면 학습이 더디고, 값이 너무 크면 튈 수도 있음.적정한 값을 유지하는 것이 좋음. \n",
        "- subsample : weark learner가 학습에 사용하는 데이터의 샘플링 비율. 기본값은 1이며 전체 학습 데이터으로 기반함.0.7이면 70%을 뜻함.\n",
        "\n",
        "\n",
        "하지만 Gradient Boosting부터는 grid Search 등을 이용해서 hyperparameter tuning 작업을 할 때 주의사항이 있음.\n",
        "\n",
        "gradient boosting부터는 하이퍼 파라미터가 매우 많기 때문에 하이퍼 파라미터 튜닝 과정에서 시간소모가 크게 작용 됨. 그래서 수행시간이 오래 걸릴 수 있음.\n",
        "\n",
        "\n",
        "\n",
        "코드 참고)\n",
        "\n",
        "- Introduction to Machine Learning with Python, Sarah Guido\n",
        "\n"
      ],
      "metadata": {
        "id": "lMT5FS-nzWRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 따로 따라 적으면서 실습은 하지 않았고 참고사항으로 코드 복붙한 내용\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3JWBtFDU2QTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=0)\n",
        "\n",
        "gbc = GradientBoostingClassifier(random_state=0) # 기본값: max_depth=3, learning_rate=0.1\n",
        "\n",
        "gbc.fit(x_train, y_train)\n",
        "\n",
        "score_train = gbc.score(x_train, y_train) # train set 정확도\n",
        "\n",
        "print('{:.3f}'.format(score_train))\n",
        "\n",
        "score_test = gbc.score(x_test, y_test) # 일반화 정확도\n",
        "\n",
        "print('{:.3f}'.format(score_test))"
      ],
      "metadata": {
        "id": "eNJ3f6k_sKcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting을 막기 위해 트리의 깊이를 줄여 pre-pruning을 강하게 함."
      ],
      "metadata": {
        "id": "SsCbxeS-zb-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbc = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
        "\n",
        "gbc.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "score_train_pre = gbc.score(x_train, y_train) # train set 정확도\n",
        "\n",
        "print('{:.3f}'.format(score_train_pre))\n",
        "\n",
        "\n",
        "score_test_pre = gbc.score(x_test, y_test) # 일반화 정확도\n",
        "\n",
        "print('{:.3f}'.format(score_test_pre))\n"
      ],
      "metadata": {
        "id": "2jGP4Qa-zasB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning_rate를 조절"
      ],
      "metadata": {
        "id": "kA2V_4dizep4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbc = GradientBoostingClassifier(random_state=0, max_depth=3, learning_rate=0.01) # 기본값 0.1\n",
        "\n",
        "gbc.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "score_train_lr = gbc.score(x_train, y_train)\n",
        "\n",
        "print('{:.3f}'.format(score_train_lr))\n",
        "\n",
        "score_test_lr = gbc.score(x_test, y_test) \n",
        "\n",
        "print('{:.3f}'.format(score_test_lr))"
      ],
      "metadata": {
        "id": "bW4_cx-azf2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 결과를 시각화"
      ],
      "metadata": {
        "id": "fMN8ykRvzhgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "score_set = np.round([score_train, score_test], 3)\n",
        "\n",
        "max_depth_score_set = np.round([score_train_pre, score_test_pre], 3)\n",
        "\n",
        "learning_rete_score_set = np.round([score_train_lr, score_test_lr], 3)\n",
        "\n",
        "\n",
        "\n",
        "index = np.arange(len(score_set))\n",
        "\n",
        "bar_width = 0.35\n",
        "\n",
        "bottom = 0.8\n",
        "\n",
        "list_set = [max_depth_score_set, learning_rete_score_set]\n",
        "\n",
        "line_set = ['--', ':']\n",
        "\n",
        "names = ['train', 'test']\n",
        "\n",
        "\n",
        "\n",
        "for i, line_set, name in zip(index, line_set, names):\n",
        "\n",
        "    plt.hlines(score_set[i], xmin=0-bar_width,\n",
        "\n",
        "               xmax=index[-1]+2*bar_width,\n",
        "\n",
        "               linestyle=line_set, label='default {}'.format(name))\n",
        "\n",
        "    plt.text(0-bar_width, score_set[i]+0.005, str(score_set[i]))\n",
        "\n",
        "\n",
        "\n",
        "plt.bar(index, max_depth_score_set, width=bar_width, label='adj max_depth')\n",
        "\n",
        "plt.bar(index+bar_width, learning_rete_score_set, width=bar_width, label='adj learning rate')\n",
        "\n",
        "for i, ts, te in zip(index, max_depth_score_set, learning_rete_score_set):\n",
        "\n",
        "    plt.text(i, (bottom+ts)/2, str(ts), horizontalalignment='center')\n",
        "\n",
        "    plt.text(i+bar_width, (bottom+te)/2, str(te), horizontalalignment='center')\n",
        "\n",
        "\n",
        "\n",
        "plt.ylim(bottom, 1.05)\n",
        "\n",
        "plt.xticks(index+bar_width/2, names)\n",
        "\n",
        "plt.ylabel('score', size=15)\n",
        "\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TfBxDeYyzimg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_depth=1일 때 특성 중요도를 시각화 하는 코드는 다음과 같습니다. "
      ],
      "metadata": {
        "id": "FAlPpNt8zkbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbc = GradientBoostingClassifier(max_depth=1, random_state=0)\n",
        "\n",
        "gbc.fit(x_train, y_train)\n",
        "\n",
        "n_feature = cancer.data.shape[1]\n",
        "\n",
        "index=np.arange(n_feature)\n",
        "\n",
        "\n",
        "\n",
        "plt.barh(index, gbc.feature_importances_, align='center')\n",
        "\n",
        "plt.yticks(index, cancer.feature_names)\n",
        "\n",
        "plt.xlabel('feature importances', size=15)\n",
        "\n",
        "plt.ylabel('feature', size=15)\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "drS28ATRzlbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost :  A scalable Tree Boosting System\n",
        "(http://dmlc.cs.washington.edu/data/pdf/XGBoostArxiv.pdf)\n",
        "\n",
        "<figure>\n",
        "<img src='https://ichi.pro/assets/images/max/724/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg' width=700 height=300>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "    <img src='\n",
        "https://ichi.pro/assets/images/max/724/1*FLshv-wVDfu-i54OqvZdHg.png' width=700 height=300>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "출처 : https://soobarkbar.tistory.com/32\n",
        "## Abstract\n",
        "- Boosting Tree는 매우 효과적이고 널리 사용되는 머신러닝 방법이다.\n",
        "- 여기서 XGBoost라는 확장 가능한(scalalbe) End-to-End Tree boosting system을 설명함.\n",
        "\n",
        "  - 종단간 기계학습(end-to-end machine learning)이라고 한다. 여기서 종단간은 처음부터 끝까지라는 의미로 데이터(입력)에서 목표한 결과(출력)를 사람이 개입 없이 얻는다는 뜻을 담고 있다\n",
        "\n",
        "\n",
        " - Sparse한 데이터에 대한 새로운 희소성 인식 알고리즘(Sparsity-Aware Algorithm)과 근사적인 트리 학습(Approximate Tree Learning)을 위한 Weighted Quantile Sketch를 제시\n",
        "\n",
        " - Scalable한 트리 부스팅 시스템을 구축하기 위해 캐쉬(cache) 액세스 패턴, 데이터 Compression 및 Sharding에 대한 insight를 제공\n",
        "\n",
        " - 이러한 것들을 결합하여, 기존 시스템보다 훨씬 더 적은 자원을 사용하며, 수십 억개 이상의 예제로 Scale함.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- 성공적인 어플리케이션으로 이끈 두 가지의 특성이 있음.\n",
        "1. 복잡한 데이터 종속성을 포착하는 효율적인(통계적인) 모델 사용.\n",
        "2. 대규모 Dataset에서 관심 모델을 학습하는 Scalable한 학습 시스템.\n",
        "\n",
        "이 논문에서는 부스팅 트리에 대해 Scalable한 머신 러닝 시스템인 XGBoost를 설명함.\n",
        "-> 성공 요인 : 모든 시나리오에서 Scalable하다.\n",
        "- 단일 머신을 이용하여 기존 솔루션보다 10배 이상 더 빠르게 실행되며, 분산되거나 제한된 메모리 상황에서도 수십억 개 이상의 예제들로 Scalable함.\n",
        "\n",
        "- XGBoost의 Scalability는 일부 중요한 시스템과 알로리즘적인 최적화 덕분ost의 Scalability는 일부 중요한 시스템과 알로리즘적인 최적화 덕분\n",
        "\n",
        "## 알고리즘 들여다 보기\n",
        "\n",
        "<figure>\n",
        "    <img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbJCZdx%2FbtqJ6dWIiTH%2FV9zZQu5tEepZkHtFMTmp5K%2Fimg.png' width=700 height=300>\n",
        "</figure>\n",
        "\n",
        "XGBoost는 Gradient Boosting 알고리즘을 기반으로 하고 있다.\n",
        "\n",
        "일반적인 Gradient Boosting과의 차이점은 Training algorithm이 개선되었고, Optimization Technique가 추가 됨\n",
        "\n",
        "출처 : https://analysisbugs.tistory.com/226?category=840322\n",
        "\n",
        "### A. Regression Task\n",
        "\n",
        "ex 문제) Drug Dosage로 Drug Effectiveness를 예측하는 Task을 가지고, 4개의 관측기가 있음.\n",
        "XGBoost는 초기 예측을 Regression, Classification 모두 Deafult로 0.5의 값을 설정\n",
        "이제 오차를 계산하고 하나의 Leaf에 전부 집어 넣음.\n",
        "\n",
        "\n",
        " - 초기값\n",
        "<figure>\n",
        "    <img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FEI93S%2FbtqKhkT2ARI%2FUBhkUKDOHBDUGSlUgPTbIk%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "\n",
        "- a) XGBoost Tree\n",
        "  - Gradient Boosting에서 Regression Tree는 Gini Index로 Tree를 구성\n",
        "  - XGBoost Tree는 Similarity Score라는 Index로 Tree를 구성\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$ Similarity ~ Score = {\\dfrac {{\\sum_{i=1}^{n}} Residual_{i}}{{\\#} ~of~ Residual+{\\lambda}}}$\n",
        "\n",
        "\n",
        "$Similarity = {\\dfrac{ {\\sum_{i=1}^{n}} Residual_{i}}{ Previous ~Probability~{\\times}~ (1-Previous Probability)+{\\lambda}}}$\n",
        "\n",
        "\n",
        "- Lambda는 Regularization Parameter로 Overfitting을 방지하는 역할\n",
        "\n",
        "-위의 케이스에 대해서 Tree를 구성해 보면(편의상 Lambda는 0으로 셋팅)\n",
        "\n",
        "$ Similarity ~ Score = {\\dfrac {(-10.5+6.5+7.5-7.50)^2}{4+0}} = 4$\n",
        "\n",
        "<figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FEI93S%2FbtqKhkT2ARI%2FUBhkUKDOHBDUGSlUgPTbIk%2Fimg.png' >\n",
        "</figure>, Similarity = 4\n",
        "\n",
        "- 모든 규칙에 대해서 Gain값을 계산한 후, 가장 높은 Gain 값을 가진 규칙이 선택.\n",
        "\n",
        "\n",
        "$ Gain = Left_{Similarity}+Right_{Similarity} - Root_{Similarity}$\n",
        "\n",
        "\n",
        "<figure>\n",
        "    <img src='https://blog.kakaocdn.net/dn/dcMwi5/btqKfp2B2Yv/yYZKToIRoz0XFik8tN9Xf1/img.png' >\n",
        "</figure>\n",
        "\n",
        "위의 경우에서는 gain이 가증 큰 (1) 규칙으로 확장됨.\n",
        "\n",
        "같은 방식으로 Gain이 음수가 되지 않을 때까지 Tree를 확장하면 다음과 같다.\n",
        "    \n",
        "\n",
        "<figure>\n",
        "    <img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQpFAI%2FbtqJ8eufZES%2F7XTDy6S9JuT8YiNNikrcn1%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "\n",
        "- 이렇게 Tree를 최대한 확장하였으면,  Pruning 작업을 해야한다.\n",
        "\n",
        "- Regularization parameter Gamma에 대해 Gain-Gamma <0 인 경우 규칙을 Pruning 한다.\n",
        "\n",
        "\n",
        "\n",
        " - $~~~~~~~~~~~~~~~~~~~{\\Large{Gain-{\\gamma} < 0}} $\n",
        "\n",
        "\n",
        "- 위 경우에서 Gamma를 130으로 정의하면, 첫 번째 규칙의 Gain이 120.33이지만 두 번 째 규칙의 Gain이 140.17이기 때문에 Pruning은 발생하지 않는다.\n",
        "\n",
        "- 반면에 Gamma를 150으로 정의하면 모든 규칙이 Pruning이 되고, 4개의 오차는 하나의 예측 값으로 예측 됨.\n",
        "\n",
        "\n",
        "** 만약, Lambda가 0이 아니면 어떻게 될 까? **\n",
        "-> Leaf에서의 Similiar Score가 달라짐.\n",
        "\n",
        "-> Gain을 다시 계산해야 됨.\n",
        "\n",
        "-> Lambda를 높이면 적은 Observation을 가지는 노드를 덜 만들어지게 하는 역할을 함.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- b) Tree Update\n",
        "-> 최족적으로 Leaf의 Observation이 어떠한 값으로 Update 되는지 알아보면,\n",
        "\n",
        "\n",
        "<figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBDXpR%2FbtqJ4H4PiDP%2FzieLWohn7fxkLCTEQyJix0%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "- Lambda = 0의 경우, 기존 Gradient Boosting과 마찬가지로 평균 값을 반환.\n",
        "\n",
        "\n",
        "<figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbmbJDk%2FbtqKiaDEsc9%2FJSLg6rGU0HiSg0RkteOE5k%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "\n",
        "예측 값에 Learning rate를 곱해준 후, 초기 예측 값에 더해주어, Update한다.\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbcafPN%2FbtqJ4HqeSCF%2FcAWOBxKJR0IhtcTBPdDK5K%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "- B. Classification Task\n",
        "\n",
        " - 위의 Drug Dosage와 문제는 같음.\n",
        "\n",
        " - a) XGBoost Tree\n",
        "  -\n",
        "  <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbgiYUL%2FbtqKfqgcm7p%2FmDtrygjXTkkUFF3SoDSh01%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "  - Regression Task에서와 마찬가지로, 계산의 편의상 Lambda = 0 으로 설정.\n",
        "  - <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcrsKh4%2FbtqKfqgcm63%2FfyhhYXPKOzW0ICCD4wDWGK%2Fimg.png' >\n",
        "</figure>\n",
        "  \n",
        "  - Gain을 최대화하는 방향으로 Tree를 확장해보자.\n",
        "\n",
        "  -  <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnsI4C%2FbtqKdGDyFZQ%2FX96iuVvbrsh3CCUolPk5B1%2Fimg.png' >\n",
        "</figure>\n",
        "\n",
        "  - Gamma를 2로 정의하면 두 규칙 모두 남는다.\n",
        "  \n",
        "\n",
        "  - Tree Update : Tree가 완성되었으므로, 각 Leaf 별로 Output을 계산해보자.\n",
        "\n",
        "  - <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F6uTvW%2FbtqKiawSSOa%2FAmR213PluM9S02i43yblm0%2Fimg.png' >\n",
        "</figure> \n",
        "\n",
        "\n",
        "- 이제 예측값의 Update는 Gradient Boosting for Classification과 마찬가지로 log(odds) 사용\n",
        "\n",
        "- $ log(odds) = log({\\dfrac {p}{1-p}}) = log({\\dfrac {0.5}{1-0.5}}) = 0 $\n",
        "\n",
        "- 예측 값에 Learning rate를 곱해준 후, 초기 예측 값에 더해주어, Update를 한다.\n",
        "\n",
        "- 예를 들어 Learning rate = 0.8일 경운 다음과 같은 업데이트가 발생.\n",
        "\n",
        "\n",
        "- <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQW3sO%2FbtqKbWfyYyd%2FRBY17aDE63u5oMhTuxRU8K%2Fimg.png' >\n",
        "</figure>  \n",
        "\n",
        "\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "5VGKwZG0znNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical details\n",
        "\n",
        "- Similarity Score와 Output Value 식을 유도\n",
        "\n",
        "\n",
        "- Gradient boosting에서 Output Value를 찾기 위해 다음 식을 최소화 하는 다음 식을 최소화 하는 Gamma를 미분으로 찾음\n",
        "\n",
        "\n",
        "- <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F0vDU4%2FbtqKiaRbDKw%2Ft6sHOgxKEQ2htIghxesJd0%2Fimg.png' >\n",
        "</figure>  \n",
        "\n",
        "- XGBoost는 Regularization term이 추가 되어, 식이 약간 변형이 됨.\n",
        "\n",
        "- <figure>\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FEv8Z3%2FbtqKhjU8Cw9%2F9fHSyTBl1k0SwONkhcCUt1%2Fimg.png' >\n",
        "</figure>  \n",
        "\n",
        "\n",
        "- Gradient boosting에서는 위를 Optimization 하기 위해서 식에 대해서 미분을 하였고, 이 때, Classification에 대해서만, Second Order Taylor Polynomail을 활용.\n",
        "\n",
        "- XGboost는 Regression 또한 Second Order Taylor Polynomail를 활용.\n",
        "\n",
        "${\\dfrac {d}{d{\\gamma}}{\\{ \\sum_{{x_i}\\in R_{ij}} L(y_{i}. F_{m-1}(x_{i}) +{\\gamma} + {\\dfrac {1}{2}}{\\lambda}{\\gamma}^2 \\}}} = 0$ 을 풀어보자.\n",
        "\n",
        "\n",
        "\n",
        "${\\dfrac {d}{d{\\gamma}} L(y_{i},F_{m-1}(x_{i})+\\gamma)} \\approx {\\dfrac{d}{dF(x_{i})}} L(y_{i},F_{m-1}(x_{i})) + {\\dfrac{d^2}{dF(x_{i})^2}} L(y_{i},F_{m-1}(x_{i})) (Gradient Boosting)$\n",
        "\n",
        "\n",
        "### Regression task의 경우 ${\\dfrac {d}{d{\\gamma}} L(y_{i},F_{m-1}(x_{i})+\\gamma)} \\approx {\\dfrac{d}{dF(x_{i})}} L(y_{i},F_{m-1}(x_{i})) + {\\dfrac{d^2}{dF(x_{i})^2}} L(y_{i},F_{m-1}(x_{i}) ) (Gradient Boosting)$\n",
        "\n",
        "\n",
        "### Regression task의 경우, 손실 함수는 MSE로 정의한다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- MSE : $L(y_{i}.F_{m-1}) = {\\dfrac {1}{2}} (y_{i} - F_{m-1}(x_{i}))^2$\n",
        "\n",
        "\n",
        "-${\\dfrac{d}{dF(x_{i})}} L(y_{i},F_{m-1}(x_{i})) = - L(y_{i},F_{m-1}(x_{i})) = Residual_{i}$\n",
        "\n",
        "-${\\dfrac{d^2}{dF(x_{i})^2}} L(y_{i},F_{m-1}(x_{i})) = 1$\n",
        "\n",
        "\n",
        "-${\\gamma} = {\\dfrac {\\sum Residual_{i}}{\\lambda + n} }$\n",
        "\n",
        "\n",
        "### Classification task의 경우, 손실 함수는 Cross-Entropy로 정의 됨.\n",
        "\n",
        "-$L(y_{i}.p_{i}) = y_{i} ~ log(p_{i}) + (1-y_{i}) ~ log(1-p_{i})$\n",
        "\n",
        "\n",
        "- ${\\dfrac{\\partial}{\\partial p_{i}}} L(y_{i},p_{i} ) = y_{i}{\\dfrac{\\partial}{\\partial p_{i}}} log(p_{i})+(1-y_{i}) {\\dfrac{\\partial}{\\partial p_{i}}} log(1-p_{i}) = {\\dfrac{y_{i}}{p_{i}}} - {\\dfrac{(1-y_{i})}{(1-p_{i})}} :    $\n",
        "\n",
        "- ${\\dfrac{d^2}{d{p_{i}}^2}} L(y_{i},p_{i} ) = -{\\dfrac {y}{p^2}} + {\\dfrac {(1-y)}{(1-p)^2}} = {\\dfrac{-y(1-p)^2 +p^2(1-y)}{p^2(1-p)^2}} = {\\dfrac{-y+2py-yp^2+p^2-p^2y}{p^2(1-p^2)}}$\n",
        "\n",
        "\n",
        "- ${\\gamma} = {\\dfrac {\\sum Residual_{i}}{\\lambda + {\\sum \\{ p_{i} \\times (1-p_{i}) \\} }} }$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5-YpVgfUzpKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGboost의 하이퍼 파리미터\n",
        "\n",
        "\n",
        "1. Parameter 종류\n",
        " - General Parameter: 전체 기능을 가이드\n",
        " - Boost Parameter: 각각의 step에서 booster 가이드\n",
        " - Learning Task Parameter: 최적화 수행 가이드\n",
        " - General Parameter\n",
        " - booster: tree 기반 모델 / 선형 모델\n",
        " - silent: 메세지 조절\n",
        " - nthread: 병렬 처리 조절\n",
        " - Boost Parameter\n",
        " - eta: Learning rate(일반적으로 0.01 - 0.2)\n",
        " - min_child_weight: min_child_weight를 기준으로 추가 분기 결정(크면  - Underfitting)\n",
        " - max_depth: Tree 깊이 수\n",
        " - max_leaf_node: 하나의 트리에서 node 개수\n",
        " - gamma: split 하기 위한 최소의 loss 감소 정의\n",
        " - subsample: 데이터 중 샘플링(0.5 - 1)\n",
        " - colsample_bytree: column 중 sampling(0.5 - 1)\n",
        " - colsample_bylevel: 각 level마다 샘플링 비율\n",
        " - lambda: L2 nrom\n",
        " - alpha: L1 norm\n",
        " - scale_pos_weight: positive, negative weight 지정 기타 등\n",
        "2. Learning Task Parameter\n",
        " - object: 목적함수 종류\n",
        " - binary:logistic(이진 분류)\n",
        " - multi:softmax(다중 분류)\n",
        " - multi-softprob(다중 확률)\n",
        "3. eval_metric: 평가 지표\n",
        " - rmse – root mean square error\n",
        " - mae – mean absolute error\n",
        " - logloss – negative log-likelihood\n",
        " - error – Binary classification error rate (0.5 threshold)\n",
        " - merror – Multiclass classification error rate\n",
        " - mlogloss – Multiclass logloss\n",
        " - auc: Area under the curve\n",
        "4. seed(Reproducibility)\n",
        "\n",
        " - np.random.seed()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "seed를 보편적으로 42를 사용하는 이유가 정말 재미있습니다.\n",
        "\n",
        "이것은 구글에 Answer to the Ultimate Question of Life, the Universe, and Everything 를 검색해보면 나오는데요.\n",
        "\n",
        "은하수를 여행하는 히치하이커를 위한 안내서 라는 책에 나온 숫자인데\n",
        "\n",
        "책 내용 중 삶, 우주, 그리고 모든 것에 대한 답을 얻으려고 deep thought를 찾아가는데 750만년 뒤 다시 자신을 찾아 달라고합니다.\n",
        "\n",
        "그리고 인간들은 750만년 뒤 다시 찾아가게 되고,  deep thought는 \"삶과 우주, 그리고 모든 것에 대한 답은 42입니다.\" 라고 합니다.\n",
        "\n",
        "저자가 말하길 42는 그냥 의미 없는 아무 숫자 였다고 하는데 이것이 많은 딥러닝 예제들에서 42를 사용하다 보니 대부분 예제에서 42를 많이 사용하는 모습을 볼 수 있습니다."
      ],
      "metadata": {
        "id": "cEHxR71IzqvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 타이타닉 문제"
      ],
      "metadata": {
        "id": "oCFrROnHzsnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#model Helpers\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "\n",
        "##Visualization\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "0kPpo--3zwLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw = pd.read_csv('/content/drive/MyDrive/titanic/train.csv')\n",
        "data_val  = pd.read_csv('/content/drive/MyDrive/titanic/test.csv')\n",
        "\n",
        "data1 = data_raw.copy(deep = True)\n",
        "\n",
        "data_cleaner = [data1, data_val]"
      ],
      "metadata": {
        "id": "ySdspmQszxp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in data_cleaner:    \n",
        "    #missing age with median\n",
        "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
        "\n",
        "    #embarked with mode\n",
        "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
        "\n",
        "    #missing fare with median\n",
        "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
        "    \n",
        "#delete the cabin feature/column and others previously stated to exclude in train dataset\n",
        "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
        "data1.drop(drop_column, axis=1, inplace = True)\n",
        "\n",
        "print(data1.isnull().sum())\n",
        "print(\"-\"*10)\n",
        "print(data_val.isnull().sum())"
      ],
      "metadata": {
        "id": "tyJvFFT9zyPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in data_cleaner:    \n",
        "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
        "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
        "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
        "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
        "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
        "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
        "#희소한 작위 명 제거\n",
        "stat_min = 10 #\n",
        "title_names = (data1['Title'].value_counts() < stat_min)\n",
        "#'작은 수'는 임의적으로, 통계상 일반적으로 쓰이는 최소값을 사용.. \n",
        "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
        "print(data1['Title'].value_counts())\n",
        "print(\"-\"*10)"
      ],
      "metadata": {
        "id": "l17tKFdEz0MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = LabelEncoder()\n",
        "for dataset in data_cleaner:    \n",
        "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
        "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
        "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
        "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
        "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
        "\n",
        "Target = ['Survived']\n",
        "\n",
        "#define x variables for original features aka feature selection\n",
        "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
        "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
        "data1_xy =  Target + data1_x\n",
        "print('Original X Y: ', data1_xy, '\\n')\n",
        "\n",
        "#define x variables for original w/bin features to remove continuous variables\n",
        "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
        "data1_xy_bin = Target + data1_x_bin\n",
        "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
        "#define x and y variables for dummy features original\n",
        "data1_dummy = pd.get_dummies(data1[data1_x])\n",
        "data1_x_dummy = data1_dummy.columns.tolist()\n",
        "#tolist() 함수를 사용하여 같은 레벨(위치)에 있는 데이터 끼리 묶어준다\n",
        "data1_xy_dummy = Target + data1_x_dummy\n",
        "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
        "\n",
        "data1_dummy.head()"
      ],
      "metadata": {
        "id": "uYyEKAZSz1Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train columns with null values: \\n', data1.isnull().sum())\n",
        "print(\"-\"*10)\n",
        "print (data1.info())\n",
        "print(\"-\"*10)\n",
        "\n",
        "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
        "print(\"-\"*10)\n",
        "print (data_val.info())\n",
        "print(\"-\"*10)\n",
        "\n",
        "data_raw.describe(include = 'all')"
      ],
      "metadata": {
        "id": "VQbXHpNfz2za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
        "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
        "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
        "\n",
        "\n",
        "print(\"Data1 Shape: {}\".format(data1.shape))\n",
        "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
        "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
        "\n",
        "train1_x_bin.head()"
      ],
      "metadata": {
        "id": "ai6zLpzGz5XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in data1_x:\n",
        "    if data1[x].dtype != 'float64' :\n",
        "        print('Survival Correlation by:', x)\n",
        "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
        "        print('-'*10, '\\n')\n",
        "        \n",
        "#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\n",
        "print(pd.crosstab(data1['Title'],data1[Target[0]]))"
      ],
      "metadata": {
        "id": "RSu1B4rKz6rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[16,12])\n",
        "\n",
        "plt.subplot(231)\n",
        "plt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\n",
        "plt.title('Fare Boxplot')\n",
        "plt.ylabel('Fare ($)')\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.boxplot(data1['Age'], showmeans = True, meanline = True)\n",
        "plt.title('Age Boxplot')\n",
        "plt.ylabel('Age (Years)')\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\n",
        "plt.title('Family Size Boxplot')\n",
        "plt.ylabel('Family Size (#)')\n",
        "\n",
        "plt.subplot(234)\n",
        "plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Fare Histogram by Survival')\n",
        "plt.xlabel('Fare ($)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(235)\n",
        "plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Age Histogram by Survival')\n",
        "plt.xlabel('Age (Years)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(236)\n",
        "plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Family Size Histogram by Survival')\n",
        "plt.xlabel('Family Size (#)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "P-JlDpiqz8Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#graph individual features by survival\n",
        "fig, saxis = plt.subplots(2, 3,figsize=(16,12))\n",
        "\n",
        "sns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\n",
        "sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\n",
        "sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n",
        "\n",
        "sns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\n",
        "sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\n",
        "sns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])"
      ],
      "metadata": {
        "id": "6Wfa30R5z9ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n",
        "\n",
        "sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\n",
        "axis1.set_title('Pclass vs Fare Survival Comparison')\n",
        "\n",
        "sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\n",
        "axis2.set_title('Pclass vs Age Survival Comparison')\n",
        "\n",
        "sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\n",
        "axis3.set_title('Pclass vs Family Size Survival Comparison')"
      ],
      "metadata": {
        "id": "dKgnTQ4Xz-y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, qaxis = plt.subplots(1,3,figsize=(14,12))\n",
        "\n",
        "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\n",
        "axis1.set_title('Sex vs Embarked Survival Comparison')\n",
        "\n",
        "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\n",
        "axis1.set_title('Sex vs Pclass Survival Comparison')\n",
        "\n",
        "sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\n",
        "axis1.set_title('Sex vs IsAlone Survival Comparison')"
      ],
      "metadata": {
        "id": "pTVZbvNr0AII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n",
        "\n",
        "#how does family size factor with sex & survival compare\n",
        "sns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n",
        "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
        "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n",
        "\n",
        "#how does class factor with sex & survival compare\n",
        "sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n",
        "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
        "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)"
      ],
      "metadata": {
        "id": "dEPox8ov0Bop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = sns.FacetGrid(data1, col = 'Embarked')\n",
        "e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\n",
        "e.add_legend()"
      ],
      "metadata": {
        "id": "yPqTo7bl0DBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\n",
        "a.map(sns.kdeplot, 'Age', shade= True )\n",
        "a.set(xlim=(0 , data1['Age'].max()))\n",
        "a.add_legend()"
      ],
      "metadata": {
        "id": "cFCZzhjV0EGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\n",
        "h.map(plt.hist, 'Age', alpha = .75)\n",
        "h.add_legend()"
      ],
      "metadata": {
        "id": "Z1HrrE1w0FGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
        "pp.set(xticklabels=[])"
      ],
      "metadata": {
        "id": "8faAeQ-o0GIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_heatmap(df):\n",
        "    _ , ax = plt.subplots(figsize =(14, 12))\n",
        "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
        "    \n",
        "    _ = sns.heatmap(\n",
        "        df.corr(), \n",
        "        cmap = colormap,\n",
        "        square=True, \n",
        "        cbar_kws={'shrink':.9 }, \n",
        "        ax=ax,\n",
        "        annot=True, \n",
        "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
        "        annot_kws={'fontsize':12 }\n",
        "    )\n",
        "    \n",
        "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
        "\n",
        "correlation_heatmap(data1)"
      ],
      "metadata": {
        "id": "_jDBeh5s0HYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### xgboost 설치하기\n",
        "\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "HIz9DrOs0JJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### lightgbm 설치하기\n",
        "\n",
        "!pip install lightgbm"
      ],
      "metadata": {
        "id": "1mKGLl2m0Nhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### catboost 설치하기\n",
        "\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "9mbpku0V0QoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
        "from sklearn import tree, linear_model, neighbors, naive_bayes, ensemble\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#Common Model Helpers\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import feature_selection\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "MLA = [\n",
        "    #Ensemble Methods\n",
        "    ensemble.AdaBoostClassifier(),\n",
        "    ensemble.BaggingClassifier(),\n",
        "    ensemble.ExtraTreesClassifier(),\n",
        "    ensemble.GradientBoostingClassifier(),\n",
        "    ensemble.RandomForestClassifier(),\n",
        " \n",
        "    #GLM\n",
        "    linear_model.LogisticRegressionCV(),\n",
        "    linear_model.RidgeClassifierCV(),\n",
        "    linear_model.SGDClassifier(),\n",
        "    \n",
        "    #Nearest Neighbor\n",
        "    KNeighborsClassifier(),\n",
        "    \n",
        "    #Trees    \n",
        "    tree.DecisionTreeClassifier(),\n",
        "    tree.ExtraTreeClassifier(),\n",
        "    XGBClassifier(verbose=0),\n",
        "    LGBMClassifier(verbose=0),\n",
        "    CatBoostClassifier(verbose=0)]\n",
        "\n",
        "\n",
        "\n",
        "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
        "#note: this is an alternative to train_test_split\n",
        "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n",
        "# run model 10x with 60/30 split intentionally leaving out 10%\n",
        "\n",
        "#create table to compare MLA metrics\n",
        "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
        "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
        "\n",
        "#create table to compare MLA predictions\n",
        "MLA_predict = data1[Target]\n",
        "\n",
        "#index through MLA and save performance to table\n",
        "row_index = 0\n",
        "for alg in MLA:\n",
        "\n",
        "    #set name and parameters\n",
        "    MLA_name = alg.__class__.__name__\n",
        "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
        "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
        "    \n",
        "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
        "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split,return_train_score=True)\n",
        "\n",
        "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
        "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
        "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
        "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
        "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
        "    \n",
        "\n",
        "    #save MLA predictions - see section 6 for usage\n",
        "    alg.fit(data1[data1_x_bin], data1[Target])\n",
        "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
        "    \n",
        "    row_index+=1\n",
        "\n",
        "    \n",
        "\n",
        "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
        "MLA_compare"
      ],
      "metadata": {
        "id": "stt0sBiy0T6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
        "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
        "\n",
        "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
        "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
        "plt.xlabel('Accuracy Score (%)')\n",
        "plt.ylabel('Algorithm')"
      ],
      "metadata": {
        "id": "Z8MnYGVy35_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance"
      ],
      "metadata": {
        "id": "FNTlXsfN37iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#IMPORTANT: This is a handmade model for learning purposes only.\n",
        "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
        "\n",
        "#coin flip model with random 1/survived 0/died\n",
        "\n",
        "#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
        "for index, row in data1.iterrows(): \n",
        "    #random number generator: https://docs.python.org/2/library/random.html\n",
        "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0\n",
        "        data1.loc[index, 'Random_Predict'] = 1    #predict survived/1 \n",
        "    else: \n",
        "        data1.loc[index, 'Random_Predict'] = 0 #predict died/0\n",
        "    \n",
        "\n",
        "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
        "#the mean of the column will then equal the accuracy\n",
        "data1['Random_Score'] = 0 #assume prediction wrong\n",
        "data1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
        "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
        "\n",
        "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
        "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
        "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))"
      ],
      "metadata": {
        "id": "OHyHWrir375Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
        "pivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\n",
        "print('Survival Decision Tree w/Female Node: \\n',pivot_female)\n",
        "\n",
        "pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\n",
        "print('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)"
      ],
      "metadata": {
        "id": "HPh7ntvq3-Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
        "def mytree(df):\n",
        "    \n",
        "    #initialize table to store predictions\n",
        "    Model = pd.DataFrame(data = {'Predict':[]})\n",
        "    male_title = ['Master'] #survived titles\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "        #Question 1: Were you on the Titanic; majority died\n",
        "        Model.loc[index, 'Predict'] = 0\n",
        "\n",
        "        #Question 2: Are you female; majority survived\n",
        "        if (df.loc[index, 'Sex'] == 'female'):\n",
        "                  Model.loc[index, 'Predict'] = 1\n",
        "\n",
        "        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n",
        "\n",
        "        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n",
        "        if ((df.loc[index, 'Sex'] == 'female') & \n",
        "            (df.loc[index, 'Pclass'] == 3) & \n",
        "            (df.loc[index, 'Embarked'] == 'S')  &\n",
        "            (df.loc[index, 'Fare'] > 8)\n",
        "\n",
        "           ):\n",
        "                  Model.loc[index, 'Predict'] = 0\n",
        "\n",
        "        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n",
        "        if ((df.loc[index, 'Sex'] == 'male') &\n",
        "            (df.loc[index, 'Title'] in male_title)\n",
        "            ):\n",
        "            Model.loc[index, 'Predict'] = 1\n",
        "        \n",
        "        \n",
        "    return Model\n",
        "\n",
        "\n",
        "#model data\n",
        "Tree_Predict = mytree(data1)\n",
        "print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n",
        "\n",
        "\n",
        "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
        "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
        "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "print(metrics.classification_report(data1['Survived'], Tree_Predict))"
      ],
      "metadata": {
        "id": "DD5H0voU3_7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot Accuracy Summary\n",
        "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "class_names = ['Dead', 'Survived']\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
        "                      title='Normalized confusion matrix')"
      ],
      "metadata": {
        "id": "gCOA5Xr94BLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base model\n",
        "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
        "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
        "dtree.fit(data1[data1_x_bin], data1[Target])\n",
        "print(base_results)\n",
        "\n",
        "print('BEFORE DT Parameters: ', dtree.get_params())\n",
        "# print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
        "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
        "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
        "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
        "print('-'*10)\n",
        "\n",
        "\n",
        "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
        "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
        "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
        "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
        "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
        "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
        "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
        "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
        "             }\n",
        "\n",
        "#print(list(model_selection.ParameterGrid(param_grid)))\n",
        "\n",
        "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
        "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
        "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
        "\n",
        "#print(tune_model.cv_results_.keys())\n",
        "#print(tune_model.cv_results_['params'])\n",
        "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
        "#print(tune_model.cv_results_['mean_train_score'])\n",
        "# print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
        "#print(tune_model.cv_results_['mean_test_score'])\n",
        "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
        "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
        "print('-'*10)\n",
        "\n",
        "\n",
        "#duplicates gridsearchcv\n",
        "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
        "\n",
        "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
        "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
        "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
        "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
        "#print('-'*10)"
      ],
      "metadata": {
        "id": "S_1qapHN4ClQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base model\n",
        "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
        "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
        "\n",
        "# print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
        "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
        "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
        "print('-'*10)\n",
        "#feature selection\n",
        "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
        "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
        "\n",
        "#transform x&y to reduced features and fit new model\n",
        "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
        "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
        "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n",
        "\n",
        "#print(dtree_rfe.grid_scores_)\n",
        "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
        "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
        "\n",
        "# print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
        "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
        "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
        "print('-'*10)\n",
        "\n",
        "\n",
        "#tune rfe model\n",
        "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
        "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
        "\n",
        "#print(rfe_tune_model.cv_results_.keys())\n",
        "#print(rfe_tune_model.cv_results_['params'])\n",
        "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
        "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
        "# print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
        "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
        "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
        "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
        "print('-'*10)"
      ],
      "metadata": {
        "id": "6muwQcaN4ECT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "import graphviz \n",
        "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
        "                                feature_names = data1_x_bin, class_names = True,\n",
        "                                filled = True, rounded = True)\n",
        "graph = graphviz.Source(dot_data) \n",
        "graph"
      ],
      "metadata": {
        "id": "dIMt-X4g4FlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
        "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
        "correlation_heatmap(MLA_predict)"
      ],
      "metadata": {
        "id": "67DkHkKm4Hgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#why choose one model, when you can pick them all with voting classifier\n",
        "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
        "vote_est = [\n",
        "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
        "    ('ada', ensemble.AdaBoostClassifier()),\n",
        "    ('bc', ensemble.BaggingClassifier()),\n",
        "    ('etc',ensemble.ExtraTreesClassifier()),\n",
        "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
        "    ('rfc', ensemble.RandomForestClassifier()),\n",
        "\n",
        "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "    ('lr', linear_model.LogisticRegressionCV()),\n",
        "    \n",
        "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "    ('bnb', naive_bayes.BernoulliNB()),\n",
        "    ('gnb', naive_bayes.GaussianNB()),\n",
        "    \n",
        "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
        "    ('knn', neighbors.KNeighborsClassifier()),\n",
        "    \n",
        "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
        "   ('xgb', XGBClassifier()),\n",
        "\n",
        "   # lightgbm : https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
        "   ('lightgbm', LGBMClassifier()),\n",
        "\n",
        "   # catboost : https://catboost.ai/en/docs/concepts/parameter-tuning\n",
        "   ('Catboost', CatBoostClassifier())\n",
        "]\n",
        "\n",
        "\n",
        "#Hard Vote or majority rules\n",
        "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
        "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
        "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
        "\n",
        "# print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
        "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
        "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
        "print('-'*10)\n",
        "\n",
        "\n",
        "#Soft Vote or weighted probabilities\n",
        "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
        "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
        "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
        "\n",
        "# print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
        "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
        "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
        "print('-'*10)"
      ],
      "metadata": {
        "id": "Pe-l_zLu4JBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 오류난곳\n",
        "#### 오류 난 이유 : 그리드에서 찾는 코드 -> 시간이 엄청 오래 걸려서 일부러 중지함\n",
        "#### 코드를 중간에 자르면 안되고 다 써야 하는 코드인데 돌리려면 4시간정도 걸림.\n",
        "#### 이번 데이터 분석에서는 너무 길고 오래 걸리니 쓰지 말고 이런게 있다 정도만 알아두기.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#WARNING: Running is very computational intensive and time expensive.\n",
        "#Code is written for experimental/developmental purposes and not production ready!\n",
        "import time\n",
        "\n",
        "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "grid_n_estimator = [10, 50, 100, 300]\n",
        "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
        "grid_learn = [.01, .03, .05, .1, .25]\n",
        "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
        "grid_min_samples = [5, 10, .03, .05, .10]\n",
        "grid_criterion = ['gini', 'entropy']\n",
        "grid_bool = [True, False]\n",
        "grid_seed = [0]\n",
        "\n",
        "\n",
        "grid_param = [\n",
        "            [{\n",
        "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
        "            'n_estimators': grid_n_estimator, #default=50\n",
        "            'learning_rate': grid_learn, #default=1\n",
        "            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n",
        "            'random_state': grid_seed\n",
        "            }],\n",
        "       \n",
        "    \n",
        "            [{\n",
        "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
        "            'n_estimators': grid_n_estimator, #default=10\n",
        "            'max_samples': grid_ratio, #default=1.0\n",
        "            'random_state': grid_seed\n",
        "             }],\n",
        "\n",
        "    \n",
        "            [{\n",
        "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
        "            'n_estimators': grid_n_estimator, #default=10\n",
        "            'criterion': grid_criterion, #default=”gini”\n",
        "            'max_depth': grid_max_depth, #default=None\n",
        "            'random_state': grid_seed\n",
        "             }],\n",
        "\n",
        "\n",
        "            [{\n",
        "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
        "            #'loss': ['deviance', 'exponential'], #default=’deviance’\n",
        "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
        "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
        "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n",
        "            'max_depth': grid_max_depth, #default=3   \n",
        "            'random_state': grid_seed\n",
        "             }],\n",
        "\n",
        "    \n",
        "            [{\n",
        "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
        "            'n_estimators': grid_n_estimator, #default=10\n",
        "            'criterion': grid_criterion, #default=”gini”\n",
        "            'max_depth': grid_max_depth, #default=None\n",
        "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
        "            'random_state': grid_seed\n",
        "             }],\n",
        "    \n",
        "            [{    \n",
        "            #GaussianProcessClassifier\n",
        "            'max_iter_predict': grid_n_estimator, #default: 100\n",
        "            'random_state': grid_seed\n",
        "            }],\n",
        "        \n",
        "    \n",
        "            [{\n",
        "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
        "            'fit_intercept': grid_bool, #default: True\n",
        "            #'penalty': ['l1','l2'],\n",
        "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
        "            'random_state': grid_seed\n",
        "             }],\n",
        "            \n",
        "    \n",
        "            [{\n",
        "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
        "            'alpha': grid_ratio, #default: 1.0\n",
        "             }],\n",
        "    \n",
        "    \n",
        "            #GaussianNB - \n",
        "            [{}],\n",
        "    \n",
        "            [{\n",
        "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
        "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
        "            'weights': ['uniform', 'distance'], #default = ‘uniform’\n",
        "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "            }],\n",
        "            \n",
        "    \n",
        "            [{\n",
        "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
        "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
        "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "            'C': [1,2,3,4,5], #default=1.0\n",
        "            'gamma': grid_ratio, #edfault: auto\n",
        "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
        "            'probability': [True],\n",
        "            'random_state': grid_seed\n",
        "             }],\n",
        "\n",
        "    \n",
        "            [{\n",
        "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "            'learning_rate': grid_learn, #default: .3\n",
        "            'max_depth': [1,2,4,6,8,10], #default 2\n",
        "            'n_estimators': grid_n_estimator, \n",
        "            'seed': grid_seed  \n",
        "             }]   \n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
        "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
        "\n",
        "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
        "    #print(param)\n",
        "    \n",
        "    \n",
        "    start = time.perf_counter()        \n",
        "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
        "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
        "    run = time.perf_counter() - start\n",
        "\n",
        "    best_param = best_search.best_params_\n",
        "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
        "    clf[1].set_params(**best_param) \n",
        "\n",
        "\n",
        "run_total = time.perf_counter() - start_total\n",
        "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
        "\n",
        "print('-'*10)"
      ],
      "metadata": {
        "id": "GkKNsYP04MLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
        "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
        "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
        "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
        "\n",
        "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
        "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
        "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
        "print('-'*10)\n",
        "\n",
        "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
        "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
        "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
        "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
        "\n",
        "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
        "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
        "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
        "print('-'*10)\n",
        "\n",
        "\n",
        "#12/31/17 tuned with data1_x_bin\n",
        "#The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n",
        "#The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n",
        "#The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n",
        "#The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n",
        "#The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n",
        "#The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n",
        "#The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n",
        "#The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n",
        "#The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n",
        "#The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n",
        "#The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n",
        "#The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n",
        "#Total optimization time was 5.56 minutes."
      ],
      "metadata": {
        "id": "5NGzhDER4Ydp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare data for modeling\n",
        "print(data_val.info())\n",
        "print(\"-\"*10)\n",
        "#data_val.sample(10)\n",
        "\n",
        "\n",
        "\n",
        "#handmade decision tree - submission score = 0.77990\n",
        "data_val['Survived'] = mytree(data_val).astype(int)\n",
        "\n",
        "\n",
        "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
        "#submit_dt = tree.DecisionTreeClassifier()\n",
        "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
        "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
        "#submit_bc = ensemble.BaggingClassifier()\n",
        "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
        "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
        "#submit_etc = ensemble.ExtraTreesClassifier()\n",
        "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
        "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
        "#submit_rfc = ensemble.RandomForestClassifier()\n",
        "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
        "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "\n",
        "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
        "#submit_abc = ensemble.AdaBoostClassifier()\n",
        "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
        "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
        "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
        "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
        "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
        "\n",
        "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
        "#submit_xgb = XGBClassifier()\n",
        "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
        "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
        "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
        "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\n",
        "#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\n",
        "data_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\n",
        "\n",
        "\n",
        "#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\n",
        "#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\n",
        "#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])"
      ],
      "metadata": {
        "id": "QMJP2qkt4Zqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------\n",
        "----------------------------------------------------\n",
        "- \n",
        "- \n",
        "- \n",
        "- \n"
      ],
      "metadata": {
        "id": "py0vgxqO2ZT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터분석 참고사항\n",
        "\n",
        "데이터분석을 잘하는 방법: \n",
        "\n",
        "여러번 시도 해 보는 수밖에 없음\n",
        "\n",
        "- 기초가 중요함. 회귀를 모르고 딥러닝을 할 수 없음\n",
        "- 복습 계속 해야함.\n",
        "- 지금까지 한건 머신러닝의 완전 처음.\n",
        "- 무조건 해야하는건 아니고 관심있으면 관심있는 분야를 먼저 하는 것이 중요\n",
        "- 머신러닝만 5개월정도 해야 현업에서 대충 쓸 수 있음\n",
        "- 바로 취직은 힘들고 2~3개월 후에 취직하는게 보통\n",
        "- 관심가는 분야의 최고 회사 입사 조건을 맞추기 위해 노력해라 그럼 그 밑 회사는 금방 갈 수 있음\n",
        "- 우선 큰 틀 -> 나중에 세부적 해도 괜찮\n",
        "- 한 7번 정도 보면 언젠가 이해하지 않을까\n",
        "- 전공이 중요하다기보단 꾸준히 공부하는게 중요\n",
        "- 코딩이 자유로워야 함 : 진짜 실력 잘 안오름\n",
        "- 매일매일 꾸준히 해야함\n",
        "\n",
        "\n",
        "- 추천해 준 책(데이터분석)에는 이론이 빠져있음.\n",
        "- 책 여러개를 사서 보는것보다 하나를 계속 돌려보자\n",
        "- 데이터분석은 잘하는사람 많아서 디폴트가 되어가고있어\n",
        "- 딥러닝 잘하는거 추천\n",
        "- \n",
        "\n",
        "\n",
        "\n",
        "강의 추천\n",
        "- 부스트코스(boost course) 무료강의 있음 : 강의 좋음\n",
        "- DCBA : 고려대학교 연구실 (유튜브)\n"
      ],
      "metadata": {
        "id": "TLMli8Md6Q3d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZW_w3lZ5YWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}